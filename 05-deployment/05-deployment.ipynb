{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bc032df-e794-4440-95b7-7058032feb0c",
   "metadata": {},
   "source": [
    "# Saving and loading the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765fd9f8-ed7a-4408-8170-fad45c37e9f5",
   "metadata": {},
   "source": [
    "Let’s take a moment to recap what we’ve accomplished so far. Before we can save a model, the crucial first step is training it. I’ve extensively covered model training in previous articles, where we explored various techniques, including K-Fold cross-validation. Below, I’ve included all the code necessary for model training. While there’s nothing new here, it serves as a useful recap of our progress.\n",
    "\n",
    "The first code snippet contains all the necessary imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a89acb07-31cb-49f7-b84b-250e3aadd50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    " \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    " \n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7687dc5-3407-418e-ae36-4a58fefb6733",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb97b06c-76e9-49e7-973c-a7412fea2b49",
   "metadata": {},
   "source": [
    "The next snippet is about data preparation, where we need to read the csv file, make the column names more homogenous, and deal with categorical and numerical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40637f71-5f95-4c67-b3e7-40eae12bcf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation\n",
    " \n",
    "df = pd.read_csv('data-week-3.csv')\n",
    " \n",
    "df.columns = df.columns.str.lower().str.replace(' ', '_')\n",
    " \n",
    "categorical_columns = list(df.dtypes[df.dtypes == 'object'].index)\n",
    " \n",
    "for c in categorical_columns:\n",
    "    df[c] = df[c].str.lower().str.replace(' ', '_')\n",
    " \n",
    "df.totalcharges = pd.to_numeric(df.totalcharges, errors='coerce')\n",
    "df.totalcharges = df.totalcharges.fillna(0)\n",
    " \n",
    "df.churn = (df.churn == 'yes').astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716eba3f-72bb-4f2e-8d97-6a972f4ecfe9",
   "metadata": {},
   "source": [
    "The next snippet is about data splitting. Again we use the train_test_split function to divide the dataset in full_train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f923a0c-144f-4209-8e04-0ea648cb212a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data splitting\n",
    " \n",
    "df_full_train, df_test = train_test_split(df, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9392ae1e-9bf8-4c53-b6b4-2287e6a43074",
   "metadata": {},
   "source": [
    "In the next snippet you see what are the numerical column names and what are the categorical column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba028109-87e0-460f-8859-d34047f38232",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical = ['tenure', 'monthlycharges', 'totalcharges']\n",
    " \n",
    "categorical = ['gender', 'seniorcitizen', 'partner', 'dependents',\n",
    "       'phoneservice', 'multiplelines', 'internetservice',\n",
    "       'onlinesecurity', 'onlinebackup', 'deviceprotection', 'techsupport',\n",
    "       'streamingtv', 'streamingmovies', 'contract', 'paperlessbilling',\n",
    "       'paymentmethod']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e46ddb-0316-4a01-ac07-77ed633eee7b",
   "metadata": {},
   "source": [
    "The next snippet is about the train function. It has three arguments – the training dataframe and the target values y_train, and the third argument is C which is a LogisticRegression parameter for our model. First step here is to create dictionaries from the categorical columns, remember the numerical columns are ignored here. Next we create a DictVectorizer instance which we need to use fit_transform function on the dictionaries. So we get the X_train. Then we create our model which is a logistic regression model, that we can use for training (fit function) based on the training data (X_train and y_train). To apply the model later we need to return the DictVectorizer and the model as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa499399-ad4b-4de6-8c72-21015dda7bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(df_train, y_train, C=1.0):\n",
    "    dicts = df_train[categorical + numerical].to_dict(orient='records')\n",
    " \n",
    "    dv = DictVectorizer(sparse=False)\n",
    "    X_train = dv.fit_transform(dicts)\n",
    " \n",
    "    model = LogisticRegression(C=C, max_iter=1000)\n",
    "    model.fit(X_train, y_train)\n",
    " \n",
    "    return dv, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93dbbe00-5e44-4168-a2f8-e376e87158f6",
   "metadata": {},
   "source": [
    "As I just mentioned in the paragraph before to use the model we need also the DictVectorizer. Both are arguments for the predict function which is show in the next snippet. Besides both arguments you also need a dataframe where we can provide a prediction for. First step here is the same like in training function, we need to get the dictionaries. This can be transformed by the DictVectorizer so we get the X, what we need to make a prediction on. What we return here is the predicted probability for churning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c87c4e05-30b8-456b-ba9b-a063fb368789",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(df, dv, model):\n",
    "     dicts = df[categorical + numerical].to_dict(orient='records')\n",
    " \n",
    "     X = dv.transform(dicts)\n",
    "     y_pred = model.predict_proba(X)[:,1]\n",
    " \n",
    "     return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18dc6fda-64ce-42fb-a8de-647083580506",
   "metadata": {},
   "source": [
    "Next snippet is to setup two parameters. The first one is the C value for the Logistic Regression model, and the ‘n_splits’ parameter tells us how many splits we’re going to use in K-Fold cross-validation. Here, we’re using 5 splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52895ec9-0bfe-433b-81c5-9fbdaf8fc08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 1.0\n",
    "n_splits = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077e8b47-b08b-4fa2-ae6a-416ca533bdef",
   "metadata": {},
   "source": [
    "Next snippet shows the implemented K-Fold cross validation, where we use the parameters from the last snippet. The for loop loops over all folds and does a training for each. After that we calculate the roc_auc_score and collect the values for each fold. At the end the mean score and the standard deviation for all folds are printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70d98bf2-b4fa-4450-bb98-78a70e3d2a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ramil/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 1000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=1000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/ramil/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 1000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=1000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/ramil/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 1000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=1000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/ramil/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 1000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=1000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=1.0 0.842 +- 0.007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ramil/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 1000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=1000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "kfold = KFold(n_splits=n_splits, shuffle=True, random_state=1)  \n",
    " \n",
    "scores = []\n",
    " \n",
    "for train_idx, val_idx in kfold.split(df_full_train):\n",
    "    df_train = df_full_train.iloc[train_idx]\n",
    "    df_val = df_full_train.iloc[val_idx]\n",
    " \n",
    "    y_train = df_train.churn.values\n",
    "    y_val = df_val.churn.values\n",
    " \n",
    "    dv, model = train(df_train, y_train, C=C)\n",
    "    y_pred = predict(df_val, dv, model)\n",
    " \n",
    "    auc = roc_auc_score(y_val, y_pred)\n",
    "    scores.append(auc)\n",
    " \n",
    "print('C=%s %.3f +- %.3f' % (C, np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12109649-af68-4547-848e-5a579da69223",
   "metadata": {},
   "source": [
    "The last snipped doesn’t show the score for each fold seperately but here you can see each value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3aa02c1-835b-4181-9970-9adeb22d702f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8446829053857807,\n",
       " 0.8453826479936432,\n",
       " 0.8333774834437085,\n",
       " 0.8347968759992326,\n",
       " 0.8517657117755952]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de23b25-21ce-4666-8b10-7786a1be81d8",
   "metadata": {},
   "source": [
    "Last step is to train the final model based on the full_train data. The steps here are similar to the steps mentioned before. First is model training, then predicting the test data, and lastly calculate the roc_auc_score. We see a value of 85.8% which is a bit higher than the average of the k-folds. But there is not a big difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "588556c1-a97e-40a5-a4ac-b949509041e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8584032088573997"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dv, model = train(df_full_train, df_full_train.churn.values, C=1.0)\n",
    "y_pred = predict(df_test, dv, model)\n",
    "y_test = df_test.churn.values\n",
    " \n",
    "auc = roc_auc_score(y_test, y_pred)\n",
    "auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bd06d5-e9b5-4a3e-aea7-111b73385d46",
   "metadata": {},
   "source": [
    "Until now the model still lives in our Jupyter notebook. So we cannot just take this model and put it in a web service. Remember we want to put this model in a web service, that the marketing service can use it to score the customers. That means now we need to save this model in order to be able to load it later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f467ea18-71ca-483c-998f-d60d95e203c6",
   "metadata": {},
   "source": [
    "## Saving the model to pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f479cdff-1501-43ad-8a51-f0f45a9a9811",
   "metadata": {},
   "source": [
    "For saving the model we’ll use pickle, what is a built in library for saving Python objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53fe1b0d-fdb0-405d-8ab3-8c4cefbf6381",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278c512b-a204-497f-aed6-bba44321e058",
   "metadata": {},
   "source": [
    "First, we need to name our model file before we can write it to a file. The following snippet demonstrates two ways of naming the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e28012f-3340-4268-8261-a3dab6c5b022",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model_C=1.0.bin'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_file = 'model_C=%s.bin' % C\n",
    "output_file\n",
    "# Output: 'model_C=1.0.bin'\n",
    " \n",
    "output_file = f'model_C={C}.bin'\n",
    "output_file\n",
    "# Output: 'model_C=1.0.bin'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bf460f-20a2-4b1f-bc77-e7c58288c7f6",
   "metadata": {},
   "source": [
    "Now we want to create a file with that file name. ‘wb’ means Write Binary. We need to save DictVectorizer and the model as well, because with just the model we’ll not be able to translate a customer into a feature matrix. Closing the file is crucial. Otherwise, we cannot be certain whether this file truly contains the content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "332dec56-09d1-4dd6-a670-bac4e190e33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_out = open(output_file, 'wb')\n",
    " \n",
    "pickle.dump((dv, model), f_out)\n",
    " \n",
    "f_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5986bde0-5530-4a7d-aca4-afde7eac037e",
   "metadata": {},
   "source": [
    "To avoid accidentally forgetting to close the file, we can use the ‘with’ statement, which ensures that the file is closed automatically. Everything we do inside the ‘with’ statement keeps the file open. However, once we exit this statement, the file is automatically closed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a86e2498-77b3-4451-a245-e25ed3927976",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_file, 'wb') as f_out:\n",
    "    pickle.dump((dv, model), f_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7681d01d-d870-4e26-87ee-93cab4fdad4b",
   "metadata": {},
   "source": [
    "## Loading the model with Pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b2141d-a010-46f9-8452-c4a8aee99c73",
   "metadata": {},
   "source": [
    "For loading the model we’ll also use pickle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e1400695-6ebc-46cb-9918-116b2e158859",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = 'model_C=1.0.bin'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d249f8-2617-421f-ac84-5d1e2dce5940",
   "metadata": {},
   "source": [
    "We also utilize the ‘with’ statement for loading the model. Here, ‘rb’ denotes Read Binary. We employ the ‘load’ function from pickle, which returns both the DictVectorizer and the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "24cd4307-9a8c-414f-96b1-2a911b569f7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DictVectorizer(sparse=False), LogisticRegression(max_iter=1000))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(model_file, 'rb') as f_in:\n",
    "    dv, model = pickle.load(f_in)\n",
    " \n",
    "dv, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a57d27-db01-40f5-993d-ba9b17bcc9ac",
   "metadata": {},
   "source": [
    "After loading the model, let’s use it to score one sample customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9dcc3446-039c-43fd-903f-3b8d4584a626",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer = {\n",
    "    'gender': 'female',\n",
    "    'seniorcitizen': 0,\n",
    "    'partner': 'yes',\n",
    "    'dependents': 'no',\n",
    "    'phoneservice': 'no',\n",
    "    'multiplelines': 'no_phone_service',\n",
    "    'internetservice': 'dsl',\n",
    "    'onlinesecurity': 'no',\n",
    "    'onlinebackup': 'yes',\n",
    "    'deviceprotection': 'no',\n",
    "    'techsupport': 'no',\n",
    "    'streamingtv': 'no',\n",
    "    'streamingmovies': 'no',\n",
    "    'contract': 'month-to-month',\n",
    "    'paperlessbilling': 'yes',\n",
    "    'paymentmethod': 'electronic_check',\n",
    "    'tenure': 1,\n",
    "    'monthlycharges': 29.85,\n",
    "    'totalcharges': 29.85\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1715cfa-0c27-4f17-b52d-48ae02a37ad7",
   "metadata": {},
   "source": [
    "Before we can apply the predict function to this customer we need to turn it into a feature matrix. The DictVectorizer expects a list of dictionaries, that’s why we create a list with one customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6a1eea75-970c-4953-8c84-1d1c888a4189",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.  ,  0.  ,  0.  ,  1.  ,  0.  ,  1.  ,  0.  ,  0.  ,  1.  ,\n",
       "         0.  ,  1.  ,  0.  ,  0.  , 29.85,  0.  ,  1.  ,  0.  ,  0.  ,\n",
       "         0.  ,  1.  ,  1.  ,  0.  ,  0.  ,  0.  ,  1.  ,  0.  ,  1.  ,\n",
       "         0.  ,  0.  ,  1.  ,  0.  ,  1.  ,  0.  ,  0.  ,  1.  ,  0.  ,\n",
       "         0.  ,  1.  ,  0.  ,  0.  ,  1.  ,  0.  ,  0.  ,  1.  , 29.85]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = dv.transform([customer])\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb90f1d-9a20-4773-b092-c461504a5229",
   "metadata": {},
   "source": [
    "We use predict function to get the probability that this particular customer is going to churn. We’re interested in the second element, so we need to set the row=0 and column=1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c55a9842-6358-4f2c-a945-5affec9d17fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.6275953527536646)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_proba(X)\n",
    "# Output: array([[0.36364158, 0.62759535]])\n",
    " \n",
    "model.predict_proba(X)[0,1]\n",
    "# Output: 0.6275953527536646"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f27d97d-0d5d-4396-afd3-0242a7fac9b9",
   "metadata": {},
   "source": [
    "## Turning our notebook into a Python script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c30f5db-5eae-457f-bb20-94667f653bd0",
   "metadata": {},
   "source": [
    "We can turn the Jupyter Notebook code into a Python file. One easy way of doing this is click on “File” -> “Download as” and then “Python (.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a3c0f9-4aee-45f8-a6ee-1fb9d86922c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
